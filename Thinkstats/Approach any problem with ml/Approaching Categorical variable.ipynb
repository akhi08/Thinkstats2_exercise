{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fca4b3-e935-43bc-9bc1-61d710caa9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3811a7ee-23a5-4be2-a4f2-e179e9630910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77896b4-af67-4422-a296-3ca2226dd4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/car-in-the-dat.zip'\n",
    "print(os.path.exists(file_path))  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1759db9b-71ea-4d30-add1-0fcd80900cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import pandas as pd\n",
    "\n",
    "    # df=pd.read_csv('/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/data/train.csv')\n",
    "\n",
    "# or-------\n",
    "\n",
    "# # Path to the ZIP folder\n",
    "# zip_path = '/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/cat-in-the-dat.zip'\n",
    "\n",
    "# # Open the zip and list contents\n",
    "# with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "#     print(\"Files in ZIP:\", z.namelist())  # View files like 'train.csv', 'test.csv'\n",
    "\n",
    "#     # Load specific CSVs\n",
    "#     with z.open('train.csv') as f:\n",
    "#         train_df = pd.read_csv(f)\n",
    "\n",
    "#     with z.open('test.csv') as f:\n",
    "#         test_df = pd.read_csv(f)\n",
    "\n",
    "# # Now you can use train_df and test_df as usual\n",
    "# # print(train_df.head())\n",
    "# # print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c99ce-39a7-4800-96ae-e2507c491b29",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03b6907-55e6-46a7-a008-8a7db41b6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={\n",
    "    \"Freezing\": 0,\n",
    "    \"Warm\": 1,\n",
    "    \"Cold\": 2,\n",
    "    \"Boiling Hot\": 3,\n",
    "    \"Hot\": 4,\n",
    "    \"Lava Hot\": 5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d48562-1cf7-46b6-96f8-5a3eb367b85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Freezing       99816\n",
       "Lava Hot       63908\n",
       "Boiling Hot    60627\n",
       "Cold           33768\n",
       "Hot            22227\n",
       "Warm           19654\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "468fbed4-1268-4ff7-83c3-6ca31e439f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:,\"ord_2\"]=train_df.ord_2.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9bf7cd7-6080-47bd-8bb1-7deba3f5a535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "0    99816\n",
       "5    63908\n",
       "3    60627\n",
       "2    33768\n",
       "4    22227\n",
       "1    19654\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733e69b-c2b1-487f-8bc6-de1119276711",
   "metadata": {},
   "source": [
    "## ~ LabelEncoder,OneHotEncoder,Binarization\n",
    "- above same encoding into numerical value can be done by LabelEncoder from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e5baa-1255-47b0-8dc9-9a09a679dc6f",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c498bab-5be5-4c5b-a64a-2d71137fe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df=pd.read_csv('/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c39969d-9144-4ea8-945b-9906092ac7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,\"ord_2\"]=df.ord_2.fillna(\"NONE\")\n",
    "# LabelEncoder from scikit-learn does not handle NaN values, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b95b182-b75f-4164-9523-3a9ec3065b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc=preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c54cadf0-1f1f-467d-a1e7-77597bb8b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit label encoder and transform values on ord_2 column\n",
    "# P.S: do not use this directly. fit first, then transform\n",
    "df.loc[:,\"ord_2\"]=lbl_enc.fit_transform(df.ord_2.values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61396b4d-433b-481f-8cd2-2a24a57ee0a2",
   "metadata": {},
   "source": [
    "# labelEncoder can be used \n",
    "• Decision trees\n",
    "• Random forest\n",
    "• Extra Trees\n",
    "• Or any kind of boosted trees model\n",
    "o XGBoost\n",
    "o GBM\n",
    "o LightGBM\n",
    "This type of encoding cannot be used in linear models, support vector machines or\n",
    "neural networks as they expect data to be normalized (or standardized)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf7ccbe4-a870-4ae3-a1e8-f5ad7e1ec73c",
   "metadata": {},
   "source": [
    "✅ What it's saying:\n",
    "1. For Tree-Based Models (e.g., Decision Trees, Random Forest, XGBoost, LightGBM):\n",
    "You can use Label Encoding or even leave NaNs in some cases.\n",
    "\n",
    "Tree models do not require one-hot or binarized encoding because they can naturally handle splits based on categorical values.\n",
    "\n",
    "That’s why it's okay to use LabelEncoder (after handling NaNs properly) or even ordinal encoding.\n",
    "\n",
    "2. For Linear Models / SVMs / Neural Networks:\n",
    "You should not use Label Encoding because it introduces an artificial ordinal relationship between categories (e.g., A=0, B=1, C=2 suggests C > B > A which may not be true).\n",
    "\n",
    "Instead, you should use One-Hot Encoding or Binarization.\n",
    "\n",
    "Binarization is converting each category into a binary format (e.g., A → [0 0 1], B → [0 1 0], C → [1 0 0]).\n",
    "\n",
    "This is often done in sparse format to save memory, since many values will be 0.\n",
    "\n",
    "❓ So is it suggesting binarization for linear models?\n",
    "Yes. Specifically, it is saying:\n",
    "\n",
    "Use binarization/one-hot encoding for models that require numerical input to be normalized:\n",
    "Linear Regression, Logistic Regression, Support Vector Machines (SVM), and Neural Networks.\n",
    "\n",
    "Continue with Label Encoding or Ordinal Encoding for tree-based models, which can handle this directly."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc854f6f-709b-4af9-b80d-fd4a41fc03ba",
   "metadata": {},
   "source": [
    "| Model Type          | Recommended Encoding     | Notes                             |\n",
    "| ------------------- | ------------------------ | --------------------------------- |\n",
    "| Decision Tree       | Label Encoding / Ordinal | Can handle NaNs                   |\n",
    "| Random Forest       | Label Encoding / Ordinal | Can handle categories             |\n",
    "| XGBoost / LightGBM  | Label Encoding / Ordinal | Best with category-aware encoding |\n",
    "| Linear Regression   | One-Hot / Binarization   | Needs numeric & normalized data   |\n",
    "| Logistic Regression | One-Hot / Binarization   | Same                              |\n",
    "| SVM                 | One-Hot / Binarization   | Same                              |\n",
    "| Neural Networks     | One-Hot / Binarization   | Same                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35a159-2770-43ee-86db-46f16262f249",
   "metadata": {},
   "source": [
    "-  checking the bytes used by different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7a8df34-fa3f-4716-bb2f-ad26247e5682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "example=np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,0,1]]\n",
    ")\n",
    "print(example.nbytes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a47e46a-ab83-4417-9d5f-1b2a756e5149",
   "metadata": {},
   "source": [
    "One way to\n",
    "represent this matrix only with ones would be some kind of dictionary method in\n",
    "which keys are indices of rows and columns and value is 1:\n",
    "══════════════════\n",
    "(0, 2) 1\n",
    "(1, 0) 1\n",
    "(2, 0) 1\n",
    "(2, 2) 1\n",
    "══════════════════"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec6c77d6-5c7b-42ad-a9b1-d520ea9cac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "example=np.array(\n",
    "    [[0, 0, 1],\n",
    "[1, 0, 0],\n",
    "[1, 0, 1]]\n",
    ")\n",
    "\n",
    "sparse_example=sparse.csr_matrix(example)\n",
    "print(sparse_example.data.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35c3e5-ea59-442f-a07d-f663e42ef1b7",
   "metadata": {},
   "source": [
    "- The total size of the sparse csr matrix is the sum of three values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3efb6518-e8a4-43a9-8c95-df9dbb7e2b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sparse_example.data.nbytes +\n",
    "sparse_example.indptr.nbytes +\n",
    "sparse_example.indices.nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1454018-0f7c-4e70-b93e-ec4b16fb7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 399997896\n",
      "Full size of sparse array: 600036848\n"
     ]
    }
   ],
   "source": [
    "# number of rows\n",
    "n_rows = 10000\n",
    "# number of columns\n",
    "n_cols = 100000\n",
    "# create random binary matrix with only 5% values as 1s\n",
    "example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\")\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "full_size = (\n",
    "sparse_example.data.nbytes +\n",
    "sparse_example.indptr.nbytes +\n",
    "sparse_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dc0d379-955f-4616-8394-145abeb6fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, dense array takes ~8000MB or approximately 8GB of memory. The sparse\n",
    "# array, on the other hand, takes only 399MB of memory.\n",
    "# And, that’s why we prefer sparse arrays over dense whenever we have a lot of zeros\n",
    "# in our features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b083ff76-3823-4e02-b2ed-4d01ad75ec1f",
   "metadata": {},
   "source": [
    "Even though the sparse representation of binarized features takes much less\n",
    "memory than its dense representation, there is another transformation for\n",
    "categorical variables that takes even less memory. This is known as One Hot\n",
    "Encoding.\n",
    "One hot encoding is a binary encoding too in the sense that there are only two\n",
    "values, 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65aa563-0337-4985-aac7-2a2b85d6479a",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31eed1d8-9f71-4ee7-830a-52b183c3b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 144\n",
      "Size of sparse array: 24\n",
      "Full size of sparse array: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# create binary matrix\n",
    "example = np.array(\n",
    "[\n",
    "[0, 0, 0, 0, 1, 0],\n",
    "[0, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 0, 0]\n",
    "]\n",
    ")\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\")\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "full_size = (\n",
    "sparse_example.data.nbytes +\n",
    "sparse_example.indptr.nbytes +\n",
    "sparse_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6830866-ba05-4739-9f2d-2611d9f7cabf",
   "metadata": {},
   "source": [
    "We see that the dense array size is much larger than the one with binarization.\n",
    "However, the size of the sparse array is much less. Let’s try this with a much larger\n",
    "array. In this example, we will use OneHotEncoder from scikit-learn to transform\n",
    "our feature array with 1001 categories into dense and sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40695d83-b6ed-4910-9072-d7d1278f3c4d",
   "metadata": {},
   "source": [
    "### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e037532-ed0e-4c17-a64c-e67b376b7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 8000000\n",
      "Full size of sparse array: 16000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# create random 1-d array with 1001 different categories (int)\n",
    "example = np.random.randint(1000, size=1000000)\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = False to get dense array\n",
    "ohe = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "# fit and transform data with dense one hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "# print size in bytes for dense array\n",
    "print(f\"Size of dense array: {ohe_example.nbytes}\")\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = True to get sparse array\n",
    "ohe = preprocessing.OneHotEncoder(sparse_output=True)\n",
    "# fit and transform data with sparse one-hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {ohe_example.data.nbytes}\")\n",
    "full_size = (\n",
    "ohe_example.data.nbytes +\n",
    "ohe_example.indptr.nbytes + ohe_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3555c-e0c9-45d8-bda1-b74f2e761aa8",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21f4ed74-f7e7-4985-b85a-f9243f58002c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         33768\n",
       "1         22227\n",
       "2         63908\n",
       "3         60627\n",
       "4         99816\n",
       "          ...  \n",
       "299995    99816\n",
       "299996    99816\n",
       "299997    60627\n",
       "299998    60627\n",
       "299999    99816\n",
       "Name: id, Length: 300000, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].transform(\"count\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050d995d-b20e-46a4-8dfe-9c65e7f69514",
   "metadata": {},
   "source": [
    "It adds a new column (or Series) that shows, for each row in df, how many rows have the same ord_2 value — by counting the number of occurrences of each value in the \"ord_2\" column using the \"id\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "706fa7e9-56ce-45d0-8309-45fed291e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>0</td>\n",
       "      <td>8692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>1</td>\n",
       "      <td>4842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>2</td>\n",
       "      <td>14284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>3</td>\n",
       "      <td>3122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>4</td>\n",
       "      <td>9074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>5</td>\n",
       "      <td>2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Expert</td>\n",
       "      <td>1</td>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Expert</td>\n",
       "      <td>2</td>\n",
       "      <td>8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Expert</td>\n",
       "      <td>3</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Expert</td>\n",
       "      <td>4</td>\n",
       "      <td>5274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Expert</td>\n",
       "      <td>5</td>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>0</td>\n",
       "      <td>15719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>1</td>\n",
       "      <td>8702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>2</td>\n",
       "      <td>25620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>3</td>\n",
       "      <td>5697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>4</td>\n",
       "      <td>16617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>5</td>\n",
       "      <td>5073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>5720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>3129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>9401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>2069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master</td>\n",
       "      <td>4</td>\n",
       "      <td>5882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Master</td>\n",
       "      <td>5</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Novice</td>\n",
       "      <td>0</td>\n",
       "      <td>25516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>14245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Novice</td>\n",
       "      <td>2</td>\n",
       "      <td>42079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Novice</td>\n",
       "      <td>3</td>\n",
       "      <td>9452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Novice</td>\n",
       "      <td>4</td>\n",
       "      <td>27061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Novice</td>\n",
       "      <td>5</td>\n",
       "      <td>8230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ord_1  ord_2  count\n",
       "0   Contributor      0   8692\n",
       "1   Contributor      1   4842\n",
       "2   Contributor      2  14284\n",
       "3   Contributor      3   3122\n",
       "4   Contributor      4   9074\n",
       "5   Contributor      5   2857\n",
       "6        Expert      0   4980\n",
       "7        Expert      1   2850\n",
       "8        Expert      2   8432\n",
       "9        Expert      3   1887\n",
       "10       Expert      4   5274\n",
       "11       Expert      5   1642\n",
       "12  Grandmaster      0  15719\n",
       "13  Grandmaster      1   8702\n",
       "14  Grandmaster      2  25620\n",
       "15  Grandmaster      3   5697\n",
       "16  Grandmaster      4  16617\n",
       "17  Grandmaster      5   5073\n",
       "18       Master      0   5720\n",
       "19       Master      1   3129\n",
       "20       Master      2   9401\n",
       "21       Master      3   2069\n",
       "22       Master      4   5882\n",
       "23       Master      5   1852\n",
       "24       Novice      0  25516\n",
       "25       Novice      1  14245\n",
       "26       Novice      2  42079\n",
       "27       Novice      3   9452\n",
       "28       Novice      4  27061\n",
       "29       Novice      5   8230"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['ord_1','ord_2'])['id'].count().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da6fae-84eb-4e94-bb87-2818fa4d418c",
   "metadata": {},
   "source": [
    "- is used to count the number of rows (based on id) for each combination of ord_1 and ord_2 values in your DataFrame.\n",
    "- .reset_index(name=\"count\")\n",
    "Resets the index (so you get a flat DataFrame) and renames the id count column to \"count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f839d48f-1a14-4fc8-a7b0-e629fafeb872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Grandmaster_1\n",
       "1    Grandmaster_3\n",
       "2         Expert_4\n",
       "3    Grandmaster_0\n",
       "4    Grandmaster_2\n",
       "Name: new feature, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new feature\n",
    "df['new feature']=df.ord_1.astype(str)+\"_\"+df.ord_2.astype(str)\n",
    "df['new feature'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75445650-e05e-4761-a88f-29c8aad21fb5",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "Whenever you get categorical variables, follow these simple steps:\n",
    "- • fill the NaN values (this is very important!)\n",
    "- • convert them to integers by applying label encoding using LabelEncoder of scikit-learn or by using a mapping dictionary. If you didn’t fill up NaN\n",
    "values with something, you might have to take care of them in this step\n",
    "- • create one-hot encoding. Yes, you can skip binarization!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7638b-395c-4576-8372-8f8795f79198",
   "metadata": {},
   "source": [
    "#### \"rare\" Category\n",
    "- A rare category is a category\n",
    "which is not seen very often and can include many different categories. You can\n",
    "also try to “predict” the unknown category by using a nearest neighbour model\n",
    "- if you predict this category, it will become one of the categories from\n",
    "the training data.\n",
    "- we can build a simple model\n",
    "that’s trained on all features except “f3”(feature column where rare value is there). Thus, you will be creating a model that\n",
    "predicts “f3” when it’s not known or not available in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b6672-19cc-4d53-9d6b-e4fbb7362f9b",
   "metadata": {},
   "source": [
    "🧠 If you design your cross-validation in such a way that it\n",
    "replicates the prediction process when you run your model on test data, then it’s\n",
    "never going to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b11df6-5a85-4bdf-bcb0-57e0d1d8626c",
   "metadata": {},
   "source": [
    "for e.g 💡 Suppose you want to concatenate training and test data, then in each\n",
    "fold you must concatenate training and validation data and also make sure that your\n",
    "validation dataset replicates the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "336a769b-4591-4682-bdbe-de250f86168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read training data\n",
    "train =pd.read_csv('/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/data/train.csv')\n",
    "#read test data\n",
    "test =pd.read_csv('/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/data/test.csv')\n",
    "# create a fake target column for test data\n",
    "# since this column doesn't exist\n",
    "test.loc[:, \"target\"] = -1\n",
    "# concatenate both training and test data\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "# make a list of features we are interested in\n",
    "# id and target is something we should not encode\n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
    "# loop over the features list\n",
    "for feat in features:\n",
    "    # create a new instance of LabelEncoder for each feature\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    # note the trick here\n",
    "    # since its categorical data, we fillna with a string\n",
    "    # and we convert all the data to string type\n",
    "    # so, no matter its int or float, its converted to string\n",
    "    # int/float but categorical!!!\n",
    "    temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
    "    # we can use fit_transform here as we do not\n",
    "    # have any extra test data that we need to\n",
    "    # transform on separately\n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    "    # split the training and test data again\n",
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05530c5b-ecaa-4675-ab3f-48940890243e",
   "metadata": {},
   "source": [
    "This trick works when you have a problem where you already have the test dataset.\n",
    "It must be noted that this trick will not work in a live setting. For example, let’s say\n",
    "you are in a company that builds a real-time bidding solution (RTB). RTB systems\n",
    "bid on every user they see online to buy ad space. The features that can be used for\n",
    "such a model may include pages viewed in a website\n",
    "A situation like this can be avoided by using an\n",
    "“unknown” category.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56ec4f24-d023-4880-a9c8-c864aa89e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Freezing       99816\n",
       "Lava Hot       63908\n",
       "Boiling Hot    60627\n",
       "Cold           33768\n",
       "Hot            22227\n",
       "Warm           19654\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df =pd.read_csv('/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/data/train.csv')\n",
    "\n",
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182ccfa-878f-4923-be45-950c316ca5a9",
   "metadata": {},
   "source": [
    "- 🧠💡 We can treat “NONE” as unknown. So, if during live testing, we get new categories\n",
    "that we have not seen before, we will mark them as “NONE”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c58c2d36-a264-4620-bbd5-b71c9524db5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0cb6d2d-672e-46e0-9334-32d8cc205644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_4\n",
       "L    19066\n",
       "G    18899\n",
       "S    18875\n",
       "A    18258\n",
       "R    16927\n",
       "Q    15925\n",
       "K    14698\n",
       "I    14644\n",
       "Z    14232\n",
       "T    14220\n",
       "V    14143\n",
       "J    12878\n",
       "P    12839\n",
       "U    12775\n",
       "H    12743\n",
       "F    11717\n",
       "E    11303\n",
       "W     9197\n",
       "Y     8490\n",
       "X     6292\n",
       "B     6169\n",
       "O     5836\n",
       "D     3974\n",
       "C     3575\n",
       "N     2166\n",
       "M      159\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b4aac-0f93-4144-8508-ad33489f7288",
   "metadata": {},
   "source": [
    "- We see that some values appear only a couple thousand times, and some appear\n",
    "almost 40000 times.\n",
    "- define our criteria for calling a value “rare”. Let’s say the requirement\n",
    "for a value being rare in this column is a count of less than 3000\n",
    "- and all missing values will be mapped to\n",
    "“NONE”.(in data set not preset but in other case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56f043d6-a5a7-4715-82ff-8e15b9aca5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.ord_4.value_counts()[df.ord_4].values<3000,'ord_4']=\"RARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "302c90af-f915-4499-a1a9-d88cdcd8a751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_4\n",
       "L       19066\n",
       "G       18899\n",
       "S       18875\n",
       "A       18258\n",
       "R       16927\n",
       "Q       15925\n",
       "K       14698\n",
       "I       14644\n",
       "Z       14232\n",
       "T       14220\n",
       "V       14143\n",
       "J       12878\n",
       "P       12839\n",
       "U       12775\n",
       "H       12743\n",
       "F       11717\n",
       "E       11303\n",
       "W        9197\n",
       "Y        8490\n",
       "X        6292\n",
       "B        6169\n",
       "O        5836\n",
       "D        3974\n",
       "C        3575\n",
       "RARE     2325\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac3f8e-eb94-4331-adb9-3c1f2caa1ca5",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfde235-49c4-42e2-b800-d80a07781fc4",
   "metadata": {},
   "source": [
    "Before going to any kind of model building, it’s essential to take care of cross-\n",
    "validation. We have already seen the label/target distribution, and we know that it\n",
    "is a binary classification problem with skewed targets. Thus, we will be using\n",
    "StratifiedKFold to split the data here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07967ceb-ee23-4fed-b407-c153fa91280b",
   "metadata": {},
   "source": [
    "#### Creat_folds for cat-in-data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22734ae7-dc6d-4e6f-9ceb-42896f6c2d4c",
   "metadata": {},
   "source": [
    "# create_folds.py\n",
    "# import pandas and model_selection module of scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from skelean import model_selection\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "if __name__ == \"__main__\":\n",
    "    # Read training data\n",
    "    df = pd.read_csv(\"/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/input/cat-in-data/train.csv\")\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # fetch labels\n",
    "    y = df.target.values\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv('input/cat-in-data/cat_train_folds.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83788e88-2469-46dd-8a52-c54c819b0a10",
   "metadata": {},
   "source": [
    "#ohe_logres.py\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"input/cat-in-data/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesn’t matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "    [df_train[features], df_valid[features]],\n",
    "    axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(auc)\n",
    "if __name__ == \"__main__\":\n",
    "# run function for fold = 0\n",
    "# we can just replace this number and\n",
    "# run this for any fold\n",
    "    run(0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "821a017b-4d29-464c-8dab-cd804582dd33",
   "metadata": {},
   "source": [
    "❯ python -W ignore ohe_logres.py\n",
    "we can error below error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282486d-2ac5-4806-bdf3-1ca1d79ccd86",
   "metadata": {},
   "source": [
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '1' '2' ... '299997' '299998' '299999']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '0' '0' ... '0' '0' '0']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '0' '0' ... '0' '0' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '0' '1' ... '0' '0' '0']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '3' ... '1' '1' '3']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['3' '1' '4' ... '1' '2' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml/mnist_classifier/src/ohe_logres.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['3' '3' '4' ... '2' '3' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
    "  df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "/Users/akhichoudhary/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
    "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
    "\n",
    "Increase the number of iterations to improve the convergence (max_iter=100).\n",
    "You might also want to scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "-- $$ 0.7916089250181116 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9e0acb0-d02e-456f-97c4-05a2cd23ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akhichoudhary/STATS/Thinkstats/Thinkstats2_exercise/Thinkstats/Approach any problem with ml\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73ff0ee7-d20e-4e06-a7bd-95d9a0a973de",
   "metadata": {},
   "source": [
    "# as error states we have not used many parameters\n",
    "# ohe_logres.py\n",
    ".\n",
    ".\n",
    ".\n",
    "# initialize Logistic Regression model\n",
    "model = linear_model.LogisticRegression()\n",
    "# fit model on training data (ohe)\n",
    "model.fit(x_train, df_train.target.values)\n",
    "# predict on validation data\n",
    "# we need the probability values as we are calculating AUC\n",
    "# we will use the probability of 1s\n",
    "valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "# get roc auc score\n",
    "auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "# print auc\n",
    "print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "if __name__ == \"__main__\":\n",
    "for fold_ in range(5):\n",
    "run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e174c-fcf1-4912-9dbf-21c843a6863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -W ignore src/ohe_logres.py\n",
    "Fold = 0, AUC = 0.7916089250181116\n",
    "Fold = 1, AUC = 0.7885835002477297\n",
    "Fold = 2, AUC = 0.7908998540691035\n",
    "Fold = 3, AUC = 0.7898881988857341\n",
    "Fold = 4, AUC = 0.7915466071107451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86791b2e-2972-410e-8346-cba6a5e54655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cdd34-b658-4d72-a10f-3fe6827acc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
