{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75432d3-8b27-452d-90d7-5696a301d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git commands\n",
    "# git status\n",
    "# git add .\n",
    "# git commit -m \"comments\"\n",
    "# git push main origin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6280c3-e4d3-41c4-a858-a8df13258cae",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "450531d2-6ac2-4121-8dc1-8573ad185816",
   "metadata": {},
   "source": [
    "If we talk about classification problems, the most common metrics used are:\n",
    "- Accuracy\n",
    "- Precision (P)\n",
    "- Recall (R)\n",
    "- F1 score (F1)\n",
    "- Area under the ROC (Receiver Operating Characteristic) curve or simply\n",
    "AUC (AUC)\n",
    "- Log loss\n",
    "- Precision at k (P@k)\n",
    "- Average precision at k (AP@k)\n",
    "- Mean average precision at k (MAP@k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7ea81-8361-4dd4-be6f-03e6884755cf",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f79b6ab0-9b91-428a-9dc8-c0c567beb539",
   "metadata": {},
   "source": [
    "When it comes to regression, the most commonly used evaluation metrics are:\n",
    "- Mean absolute error (MAE)\n",
    "- Mean squared error (MSE)\n",
    "- Root mean squared error (RMSE)\n",
    "- Root mean squared logarithmic error (RMSLE)\n",
    "- Mean percentage error (MPE)\n",
    "- Mean absolute percentage error (MAPE)\n",
    "- R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcbc33-302b-4532-9dda-ca6fdef13450",
   "metadata": {},
   "source": [
    "#### E.g Accuracy Metrics for classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21451f6e-8fda-4e3f-9959-bd8a5ee05927",
   "metadata": {},
   "source": [
    "- Accuracy \n",
    " : defines how accurate your model is\n",
    "-- When we have an equal number of positive and negative samples in a binary\n",
    "classification metric, we generally use accuracy, precision, recall and f1\n",
    "-- if dataset is skewed, i.e., the number\n",
    "of samples in one class outnumber the number of samples in other class by a lot. In\n",
    "these kinds of cases, it is not advisable to use accuracy as an evaluation metric as it\n",
    "is not representative of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8275e9-4e7d-43f7-80c8-eb5a7b19a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: accuracy score\n",
    "    \"\"\"\n",
    "# initialize a simple counter for correct predictions\n",
    "    correct_counter = 0\n",
    "# loop over all elements of y_true\n",
    "# and y_pred \"together\"\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "# if prediction is equal to truth, increase the counter\n",
    "            correct_counter += 1\n",
    "# return accuracy\n",
    "# which is correct predictions over the number of samples\n",
    "    return correct_counter / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b938f-514f-43a2-9772-513e1d75da70",
   "metadata": {},
   "source": [
    "- using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c52e46-3e82-47df-87a6-0cc11e9eaaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "metrics.accuracy_score(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4c2cf-2618-40f9-bf71-2b8458cd6192",
   "metadata": {},
   "source": [
    "#### E.g Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43d6a3-a908-4086-8eb6-51043d23891f",
   "metadata": {},
   "source": [
    "- True +(TP) >>\n",
    "- - Given an image, if your model predicts the image has pneumothorax, and the actual target for that image has pneumothorax,it is\n",
    "considered a true positive.\n",
    "\n",
    "- True -(TN) >>\n",
    "- - Given an image, if your model predicts that the image does not\n",
    "have pneumothorax and the actual target says that it is a non-pneumothorax image,\n",
    "it is considered a true negative.\n",
    " \n",
    "- In simple words, if your model correctly predicts positive class, it is true positive,\n",
    "and if your model accurately predicts negative class, it is a true negative.\n",
    "\n",
    "- False +(FP) >>\n",
    "- - Given an image, if your model predicts pneumothorax and the\n",
    "actual target for that image is non- pneumothorax, it a false positive.\n",
    "\n",
    "- False -(FN) >>\n",
    "- - False negative (FN): Given an image, if your model predicts non-pneumothorax\n",
    "and the actual target for that image is pneumothorax, it is a false negative.\n",
    "\n",
    "- if your model incorrectly (or falsely) predicts positive class, it is\n",
    "a false positive. If your model incorrectly (or falsely) predicts negative class, it is a\n",
    "false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44133cc-2f35-474b-805b-7d84544b366b",
   "metadata": {},
   "source": [
    "Hint ðŸ§  \n",
    "The next word from True _ or False _ will be your what your model predicted\n",
    "- Positive(+) for class assigned,\n",
    "- Negative(-) for other class assigned,\n",
    "- and if its True it predicted what was there in actual,\n",
    "- and if its False it predicted opposite to what was there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a808dd-828b-45af-abfa-ed3312cfee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of true positives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "        \n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of true negatives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "        \n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of false positives\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "    \n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of false negatives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867016ba-b194-4847-9717-d81a3403da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "print(true_positive(l1, l2))\n",
    "print(false_positive(l1, l2))\n",
    "print(false_negative(l1, l2))\n",
    "print(true_negative(l1, l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d8fccb-839c-45a7-931c-ce738b44504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_v2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy using tp/tn/fp/fn\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: accuracy score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d42bc5-d5d9-4cdb-88c1-71672d6512ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "0.625\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# Cross checking correctness\n",
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "print(accuracy(l1, l2))\n",
    "print(accuracy_v2(l1, l2))\n",
    "print(metrics.accuracy_score(l1, l2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb28181-3bea-4043-b695-84032f3f2fa1",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "- Precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1e58e36-73f9-42bf-b66b-ed35ee4f2839",
   "metadata": {},
   "source": [
    "Letâ€™s say we make a new model on the new skewed dataset and our model correctly\n",
    "identified 80 non-pneumothorax out of 90 and 8 pneumothorax out of 10. Thus, we\n",
    "identify 88 images out of 100 successfully. The accuracy is, therefore, 0.88 or 88%.\n",
    "But, out of these 100 samples, 10 non-pneumothorax images are misclassified as\n",
    "having pneumothorax and 2 pneumothorax are misclassified as not having\n",
    "pneumothorax.\n",
    "Thus, we have:\n",
    "- TP : 8\n",
    "- TN: 80\n",
    "- FP: 10\n",
    "- FN: 2\n",
    "So, our precision is 8 / (8 + 10) = 0.444. This means our model is correct 44.4%\n",
    "times when itâ€™s trying to identify positive samples (pneumothorax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a251e58-ddc4-49f7-8e4e-e1d9077f28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: precision score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45627c3-69cd-47a5-a67f-5590a0937426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
